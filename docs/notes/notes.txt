Switching between languages
===================

General:
fns.php, lines 32ff - language codes
rewrite_utterances.php, line 105 - set default language
tex/generate_expex.php, lines 65ff - set default language

Conversions: 
fns.php, line 739 (update_langids) - set indeterminate language
fns.php, line 739 (update_langids) - also set secondary language denoted by @s - IMPORTANT!!!
(Deprecated - utils/sed_convert_es - set secondary language, @s)
import_and_convert.php - uncomment the appropriate conversion script
convert_xx_to_precode.php - change the output location at the end of the script

The above is now deprecated, since the autoglosser will detect the most-frequent language by inspecting the file header.

MOR:
fns.php, lineclean_surface - uncomment Miami codes to ignore items in <> - this only needs to be done for MOR


Converting
=======

The new system is to import, write out with all tags, import that, then write out in the precode format.
Where words have not been given a langid, you can add one by importing, and then comparing each word that is not punctuation against the relevant dictionary:
update zeledon7_cgwords set langid='eng' where surface not in (select surface from eslist) and langid!='999'
This takes around 1m11s to run for a 3,764-word file.  Add another clause for indeterminates if they have been marked.  If not, use:
update zeledon7_cgwords set langid='eng&spa' where surface~'^[A-Z]'

Note that the above may tag words such as do, a, come, me, etc in the wrong language, so it may be preferable to add a clause to the query to omit them.  Also, it will tag Spanish verbs with clitic pronouns as English because they are not in the Spanish dictionary.

Remember to change the primary language in convert_primary_lg.php to the relevant language you need.


Generation
=======

Remember that the outputs in inputs/<corpus>/autoglossed is only there as a way to ease maintenance of the Git tree - the files there are copied from the outputs folders.  The latter should always be the first port-of-call, since if the import has not completed for any reason the two lines at the bottom of sh_run_do_everthing (which copy the finished files from the outputs folders into the inputs/autoglossed folder) will not have run, so the files in inputs/autoglossed will be older copies.


Difference loom
==========

A possible way to check easily for regressions as the CG grammar is improved.
Copy existing outputs into a backup dir.  ?or rename them?
Autogloss as normal.  ?above could be done by adding a new line to sh_run_do_everything moving the existing files into a prev dir?
Run a loop across the files: diff -u file.cha prev/file.cha
The unified (-u) format can then be parsed to put a strikethrough on - lines, and a highlight on + lines.


Manual edit system
============

When nearing publication, the glosses may need to be manually corrected.  We need to keep track of those edits, so that if the files are re-glossed, the fixes can be re-applied, and we don't have to go through the manual correction process again.

Re-edit the file - change CG rules, add items to the dictionary, add or-squashing rules, regenerate the file.

Review the new file against the previously-printed hand-marked version.
Wherever the table entry still requires manual editing, enter the corrections into a new fix column 
Copy all records requiring fixes into a prepub table - gather_fixes.php
(Note that we need to disable regeneration of the prepub table (created via create_prepub.php) once it is first made, since that will erase all the laboriously-gathered fixes.)

(Note also that we need to find a way of not adding, or deleting once added, any duplicate fix lines from subsequent correction of the file.)
delete from forum_prepub where id not in (select min(pp.id) from forum_prepub as pp group by pp.utterance_id, pp.location, pp.surface)

Any time the file is regenerated in the future, add a hook to run the prepub table against the output, replicating the manual changes for file, utterance_id, location, and surface - apply_prepub.php

This will not work if one of the criteria is changed - so if the surface word has been changed, or new material has been added at a particular place, the criteria will not match, and the fix will not be applied.
This is an argument for only running this at the very end, once the text is pretty settled (ie after typo-corrections and proof-reading).
When the fix cannot be applied, add an invalid marker in the nf (not found) column of the prepub table.
Amend the file table at these locations, and re-export amendments.  Add to prepub if 4 criteria are not already there, and then delete from prepub where invalid marker exists.

create_cgwords now sets up an additional field called "fix", which will hold any manual changes needed.
gather_fixes.php gathers the manual changes and puts them into a prepub table.
Calls create_prepub.php to generate the empty table.
apply_prepub.php injects the fixes into the cgwords auto field.





or-squashing
========

Collect all instances of [or]s from the files.
Review, group, etc, and add a preferred option for all of them.
Add a hook to read this table and copy the preferred option to the file table.


Unknowns
=======

insert into cylist (surface, lemma, enlemma, clar, pos, gender, number, tense, notes, extra) select surface, lemma, enlemma, clar, pos, gender, number, tense, notes, extra from patagonia_cym_unknowns_uniq


Remove %aut line
===========

sed -i '/%aut/ d' path/to/file.cha


Import one field into the utterances table
==========================

update sastre5_cgutterances set eng=(select dra from mysastre5_cgutterances where mysastre5_cgutterances.utterance_id=sastre5_cgutterances.utterance_id)


Possible alternative import of the surface line
============================

Split surface at spaces
For each inter-space item
	if it occurs in a table of non-word items, insert into the table and set an item-type field to nw (non-word)
	if not, insert into the table and set an item-type field to w (word)
Add language tags only where item-type=w
Add the item again in a cleaned form (eg without brackets)
When adding glosses, split at spaces
	if the location to which the item will be saved has item-type=nw, increment the location number - repeat as necessary


List words that do not have gloss entries:
==========================

select welsh, count(welsh) from stammers4_cgwords where gloss is null group by welsh order by count desc
but this is not accurate, because manual glosses have been added - this doesn't mean that the items are in cylist

CREATE TABLE cylist_add (
    id integer NOT NULL,
    surface character varying(100),
    lemma character varying(100),
    pos character varying(20),
    gender character varying(20),
    num character varying(50),
    tense character varying(100),
    reg character varying(50),
    enlemma character varying(100),
    mutation character varying(20)
);
CREATE SEQUENCE cylist_add_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MAXVALUE
    NO MINVALUE
    CACHE 1;
ALTER SEQUENCE cylist_add_id_seq OWNED BY cylist.id;
ALTER TABLE cylist_add ALTER COLUMN id SET DEFAULT nextval('cylist_add_id_seq'::regclass);
ALTER TABLE ONLY cylist_add ADD CONSTRAINT cylist_add_pkey PRIMARY KEY (id);
CREATE INDEX cylist_add_surface ON cylist_add USING btree (surface);

egrep 'unk' outputs/stammers4_cg_applied.txt > unk.txt

Adding new entries:
cylist has 416862 entries
delete from cylist where id > 416862
insert into cylist (surface, lemma, pos, gender, num, tense, reg, enlemma, mutation) select surface, lemma, pos, gender, num, tense, reg, enlemma, mutation from cylist_add

fargain
broblem
bity
dop
drombone
fass
tagged as @0, but by definition these cannot be English, since they are mutated



Checking write_cgautogloss.php output
=======================
select w.utterance_id, w.location, w.mainlang, w.gloss, f.utterance_id as autoutt, f.location as autoloc, f.surface, f.lemma, f.tags from patagonia2_cgwords w, patagonia2_cgfinished f where w.utterance_id=f.utterance_id and w.location=f.location order by w.utterance_id, w.location




Accuracy and coverage:
==============
select surface, gls, auto , count(surface) from stammers4_cgwords where surface~'[.!?]' group by surface, gls, auto  order by surface
select surface, gls, auto, count(surface) from stammers4_cgwords where auto~'\\.N\\.' group by surface, gls, auto  order by surface
select surface, mor, auto, count(surface) from zeledon14m_cgwords where auto~'\\.N\\.' group by surface, mor, auto  order by surface
select count(*) from stammers4_cgwords where gls!~'[A-Z]'
select surface, gls, auto from stammers4_cgwords where gls!~'[A-Z]' group by surface, gls, auto order by surface
select surface, count(surface), mor, auto from zeledon14m_cgwords where mor~'^n' group by surface, mor, auto order by surface


Collecting unknown words
select lemma, count(lemma) from patagonia6_cgfinished where pos='m' group by lemma order by lemma (names)
select lemma, count(lemma) from patagonia6_cgfinished where pos='u' group by lemma order by lemma (unknown)


Log unknown words:
=============
select langid, surface, count(surface) from herring1_cgwords where auto='unk' group by langid, surface order by langid, surface

create table pat_unk as select langid, surface, count(surface) from herring1_cgwords where auto='unk' group by langid, surface order by langid, surface


Unknown words in Miami/Patagonia:
======================

You have to do a full import and autogloss so that you can select the unknown words in each language.  These then need to be added to the relevant dictionary.  Then add the indeterminates.

select surface, count(*), auto, langid from patagonia6_cgwords group by surface, auto, langid order by surface, auto, langid
select surface, count(*), auto, langid from patagonia2_cgwords where auto='unk' group by surface, auto, langid order by surface, auto, langid
select count(*) from patagonia2_cgwords
select * from patagonia6_cgwords where auto ~ '\\[or\\]'

Run create_cy_unknowns filename to create a table populated with the unk(nown) items from the patagonia2 table.
Run merge_cy_unknowns filename to merge them into cylist.
Re-autogloss.

Get all non-main language words (including indeterminates):
select surface, count(surface),langid from patagonia6_cgwords where langid!='cym' and langid!='999' and surface!~'^[A-Z]' group by surface, langid order by langid, surface
Note that these may not be unknown words (ie not in the respective language dictionary).

Get all indeterminate words:
select surface, count(surface),langid from patagonia6_cgwords where langid~'&' and surface!~'^[A-Z]' group by surface, langid order by langid, surface

Add indeterminate items to zerolist:
empty zerolist
insert into zerolist (surface) select surface from patagonia6_cgwords where langid~'&' and surface!~'^[A-Z]' group by surface
Then add them to cylist:
insert into cylist (surface, lemma, enlemma, clar, pos, gender, number, tense, notes, extra) select surface, lemma, enlemma, clar, pos, gender, number, tense, notes, extra from zerolist

Get Spanish unknowns:
select surface, count(surface) from patagonia6_cgwords where auto='unk' and langid='es' group by surface order by surface

Get all unknowns
select langid, surface, count(surface) from patagonia6_cgwords where auto='unk' group by langid, surface order by langid, surface


Unknown words in Siarad
================

select surface, count(*), auto, gls, langid from stammers4_cgwords group by surface, auto, gls, langid order by surface, auto, gls, langid
select langid, surface, count(surface) from stammers4_cgwords where auto='unk' group by langid, surface order by langid, surface


[or] words
======

select surface, count(surface) from stammers9_cgwords where auto ~'\\\[or\\\]' group by surface order by surface


Timing of import/autogloss - words
======================

select count(*) from filename_cgwords where langid !='999'


patagonia1: 1m10.177s - 1386
patagonia6: 1m20.736s/2m49.845s - 4862
patagonia3: 4m16.508s - 4710



Siarad:
stammers4: 5m21.520s - 4554







Select tablenames
============

select tablename from pg_tables where tablename !~'(pg_|sql_|_sp)' and tablename ~'_'




select * from patagonia1_cgfinished order by utterance, location



Generating cylist
==========
[Run utils/cylist/pluralise.php on canonical.  This will fill the plurals table.]  NO!  Not required - the current version of canonical already has these plurals added.
Run utils/cylist/collect.php.  This will gather all entries from 4 separate tables (canonical, berfau, vplus, virreg) into eurfa_nmni.  The last section of the script writes the lemmas based either on the surface or (in the case of a plural surface) the sorp.
Run utils/cylist/gbl.php.  This will convert the gbl table into the more compact eurfa_gbl.
update eurfa_gbl set tense='pres' where tense='present';
update eurfa_gbl set tense='cond' where tense='conditional';
update eurfa_gbl set tense='dep' where tense='dependent';
update eurfa_gbl set tense='fut' where tense='future';
update eurfa_gbl set tense='imper' where tense='imperative';
update eurfa_gbl set tense='imperf' where tense='imperfect';
update eurfa_gbl set tense='pastsubj' where tense='past subjunctive';
update eurfa_gbl set tense='pluperf' where tense='pluperfect';
update eurfa_gbl set tense='subj' where tense='subjunctive';
update eurfa_gbl set number='1s' where number='1singular';
update eurfa_gbl set number='2s' where number='2singular';
update eurfa_gbl set number='3s' where number='3singular';
update eurfa_gbl set number='1p' where number='1plural';
update eurfa_gbl set number='1p' where number='1plural';
update eurfa_gbl set number='2p' where number='2plural';
update eurfa_gbl set number='3p' where number='3plural';
update eurfa_gbl set number='0' where number='impers';
update eurfa_gbl set number='0' where number='impersonal';
Run utils/cylist/combine.php to combine eurfa_gbl and eurfa_nmni into cylist, ready for use in the autoglosser.
Change yn to stative.
Set indexes on surface, lemma, enlemma, pos, gender, number and tense.

It is vitally important to set indexes on all relevant fields on the dictionary database table.  Without indexes, the cohort-writing for Patagonia1 on a dictionary that included mutated forms took 48s.  On a dictionary that did not include mutated forms, where the words were demutated and every demutation looked up on the fly, took 340s.  On the same dictionary, but with the demutated words only being looked up where they differed from the surface word, the time was 159s.  However, with indexes set on all the main columns, the total time was slashed to 8s. 

cylist POS tags (these need to be revised):
a       adjective
ac     adjective cardinal
am    adjective comparative
ao     adjective ordinal
ap     adjective plural
b       adverb
c       conjunction
e       exclamation
h       phrase
i        interjection
n       noun
np     placename
p       preposition
pn     placename
r        pronoun
s       stative
t       article
v       verb
vh     phrasal verb
x       particle


tense tags (eslist and cylist combined):  
cond
dep
fut 
futsubj 
imper   
imperf  
imperfsubj  
infin   
past    
pastpart   
pastsubj 
pluperf
pres    
prespart    
pressubj    
subj    



Creating enlist
=========
id
surface 100
lemma 100
enlemma 100
pos 20
gender 20
number 50
tense 100
notes 50

Use the Moby part-of-speech database from aspell.
N   Noun
p   Plural
h   Noun Phrase
V   Verb (usu participle)
t   Verb (transitive)
i   Verb (intransitive)
A   Adjective
v   Adverb
C   Conjunction
P   Preposition
!   Interjection
r   Pronoun
D   Definite Article
I   Indefinite Article
o   Nominative
Rename part-of-speech.txt to part-of-speech.csv, and create table enpos with fields surface and pos
Import using: \copy enpos from '/home/kevin/autoglosser/dbdevel/pos/part-of-speech.csv' with delimiter '\t'
Filter out the WordNet stuff: insert into enlist(surface, pos) select surface, pos from enpos where pos!~'^\\|'
Remove capitalised words: delete from enlist where surface ~ '^[A-Z]'
Remove noun phrases (h): delete from enlist where pos='h'
Remove multiwords: delete from enlist where surface~' '
Check pos values: select pos, count(pos) from enlist group by pos order by count desc
The multiple ones need to be reduced - many are incorrect anyway: select surface, pos from enlist where pos='!N'
We don't want to lose info - eg "look" is both a verb and a noun: select surface, pos from enlist where surface='look'
However, the simplest thing is probably to take just the first entry in the POS string, and put it in a new field:
update enlist set mypos = substring(pos from '^.')
select pos, mypos, count(pos) from enlist group by pos, mypos order by count desc
select mypos, count(mypos) from enlist group by mypos order by count desc
Start filling out the other fields and rationalising the POS tags:
update enlist set tense='prespart' where surface~'ing$' and mypos='V'
check: select * from enlist where mypos='V' limit 500
update enlist set tense='prespart' where surface~'ing$' and mypos='V'
update enlist set tense='pastpart' where surface~'ed$' and mypos='V'
update enlist set tense='infin' where tense is null and mypos='V'
update enlist set mypos='v' where mypos='V'
update enlist set tense='prespart' where surface~'ing$' and mypos='t'
update enlist set tense='pastpart' where surface~'ed$' and mypos='t'
update enlist set tense='infin' where tense is null and mypos='t'
update enlist set mypos='v' where mypos='t'
update enlist set tense='prespart' where surface~'ing$' and mypos='i'
update enlist set tense='pastpart' where surface~'ed$' and mypos='i'
update enlist set tense='infin' where tense is null and mypos='i'
update enlist set mypos='v' where mypos='i'
update enlist set mypos='n' where mypos='N'
update enlist set number='s' where number is null and mypos='n'
update enlist set mypos='n', number='p' where number is null and mypos='p'
update enlist set mypos='i' where mypos='!'
update enlist set mypos='a' where mypos='A'
update enlist set mypos='c' where mypos='C'
update enlist set mypos='a' where mypos='h'
update enlist set mypos='p' where mypos='P'
update enlist set mypos='a' where mypos='D'
Once done, rename pos to oldpos, and mypos to pos.
Oops - forgot adverbs (v):
select * from enlist where substring(oldpos from '^.') = 'v' order by surface
update enlist set pos='b' where substring(oldpos from '^.')='v' and pos='v'


Generate other entries:
copy infin to pres.12s123p
copy ing A to prespart
copy ed A to past
copy ed V to past
copy ed V to A
handle genitive 's on the fly?
and what about others like -ing?

create table enlist_add as select * from enlist where tense='infin'
insert into enlist_add select * from enlist where surface~'ing$' and pos='a'
insert into enlist_add select * from enlist where surface~'ed$' and pos='a'
update enlist_add set tense='pres', number='12s123p' where tense='infin'
update enlist_add set pos='v', tense='prespart' where surface~'ing$' and pos='a'
update enlist_add set pos='v', tense='past' where surface~'ed$' and pos='a'
insert into enlist_add select * from enlist where surface~'ed$' and pos='v'
update enlist_add set tense='past' where surface~'ed$' and pos='v' and tense='pastpart'
insert into enlist_add select * from enlist where surface~'ed$' and pos='v'
update eslist_add set tense=NULL, pos='a' where surface~'ed$' and pos='v' and tense='pastpart'
insert into enlist(surface, oldpos, enlemma, pos, gender, number, tense, notes, lemma) select surface, oldpos, enlemma, pos, gender, number, tense, notes, lemma from enlist_add
Many of the above removed again under the new system of on-the-fly segmenting



Some strange garbage in this list:
fam, gaster, 



Google lookup
========
Use the wrapper available here:
http://www.codediesel.com/php/google-translation-php-wrapper
create table gt as 
select langid, surface, count(surface) from herring2_cgwords where auto='unk' and langid='3' group by langid, surface order by langid, surface
insert into declit select langid, surface, count(surface) from herring1_cgwords where auto='unk' and langid='2' group by langid, surface order by langid, surface
select * from gt where lookup!=''

herring7 (english) - decliticisation will handle perhaps 98% of unknowns
herring2 (spanish) - gt will handle perhaps 75% of unknowns

select distinct * from enlist where surface in (select lookup from gt) order by surface
could be used to fill out many of the pos fields from the english dictionary

But under the new TOU, can't use Google Translate for free software ....






Testun
====
select mainlang, gloss, count(gloss) from deuchar1_cgwords  where mainlang !~ '[A-Z]' and mainlang !~ '[!\?\.]' group by mainlang, gloss order by mainlang, gloss
select surface, auto, count(auto) from patagonia6_cgwords where auto != 'name' and auto != 'unk' and auto !~'[or]' group by surface, auto order by surface, auto






Cognate as percent of total items
====================
select filename, langid, mainlang, count(mainlang) from deuchar1_cgwords where langid!='999' group by mainlang, langid, filename order by langid, mainlang
create table cognates_list as select filename, langid, mainlang, count(mainlang) from smith1_cgwords where langid!='999' group by mainlang, langid, filename order by langid, mainlang
insert into cognates_list select filename, langid, mainlang, count(mainlang) from roberts1_cgwords where langid!='999' group by mainlang, langid, filename order by langid, mainlang
update cognates_list set langid='1' where langid=''
update cognates_list set langid='2' where langid='s'
update cognates_list set langid='0' where langid='zh'
select langid, mainlang, count(mainlang) from cognates_list group by mainlang, langid order by langid, mainlang
(note that count(mainlang) in the above gives the number of records with that mainlang across all the files - so for "oedd" you get a figure of 16, because it occurs in all 16 files; each of those records has its own count of the number of times "oedd" occurs in that file 

Tokens
select sum(count) from cognates_list
select sum(count) from cognates_list where langid='0'
select sum(count) from cognates_list where langid='1'
select sum(count) from cognates_list where langid='2'
select sum(count) from cognates_list where langid not in ('0','1','2')

Types
select count(distinct mainlang) from cognates_list where mainlang='oedd'
select count(distinct mainlang) from cognates_list where langid='0'
select count(distinct mainlang) from cognates_list where langid='1'
select count(distinct mainlang) from cognates_list where langid='2'
select count(distinct mainlang) from cognates_list where langid not in ('0','1','2')

create table cognates_diana as select * from cognates_list where mainlang in (select cognate from di_cognates) group by mainlang, count, langid, filename order by mainlang, filename, langid, count
select langid, mainlang, sum(count) from cognates_diana group by mainlang, langid order by langid, mainlang
select langid, mainlang, sum(count) from cognates_diana where langid!='0' group by mainlang, langid order by langid, mainlang




Finding "good" examples of autoglossing:
========================
select * from stammers4_cgutterances where utterance_id in (select utterance_id from stammers4_cgwords where auto !~'unk' and auto !~'\[or\]' and langid='2')




CG issues
======

Treatment of "that"
"It's that 'that' that is tricky"
"... but my brother was telling me that law that came about with the homestead exemption ..."

Clause marking via commas would help:
"hey baby are you back?"
There is probably an upturn on baby, hence a comma would reflect this, or if the default downturn, a slight pause after baby, so again a comma would reflect this.  However, this would play havoc with the marking .... No - split if off as we do for periods, etc, and enter it in its own slot.  It could then be used to create context for the CG rules.
New rules seem to deal with some of this.

The comp.ag tag needs to be refined in en_lookup.php.  The comp tag should only be applied to adjectives, and the ag tag to verbs, and perhaps nouns.
if $prseg2 contains comp.ag
	if $pos=adj
		$prseg2=comp
		fwrite($fp, $entry)
	if $pos=v|n
		$prseg=ag
		fwrite($fp, $entry)
	else 
		$foundclitics=0
		break
else fwrite($fp, $entry)
This should work, but the CG rules will also need changing a bit.  Therefore leave it until later.

Consider making "every" a det instead of an adj.


Main issues Spanish:
"<empieza>"
	"empezar" {465,13} [es] v 23s pres :start: [46413]
	"empezar" {465,13} [es] v 2s imper :start: [46412]
Distinguishing between pres and imper.

"<salimos>"
	"salir" {576,4} [es] v 1p past :exit: [104450]
	"salir" {576,4} [es] v 1p pres :exit: [104451]
Distinguishing between pres and past.

empujándola - not segmented. ? accent?


Both:
Need to handle capitalised words more gracefully, eg Thank God!




Conversation profiles:
=============
Run testprofile php - this creates a table called <filename>_profile.
delete from stammers4_profile where per0=0 and per1=0 and per2=0  # get rid of any lines where all the cells are 0
select * from sastre1_profile where per0+per2+per3<100 order by utterance_id  # find places where a word is untagged or a langid is missing - these produce white sections on the profile, and are usually due to typos; on sastre1, incredibly, there were 5 typos that had earlier been missed
Export the table to csv and delete columns to leave only the three percentage columns.
Run R to create an image:
png(filename="/home/kevin/autoglosser/profile/sastre1.png", height=600, width=1000, bg="white")
par(lty=0)  # Note that you have to specify this for the png even if the console already has it specified
perprofile<-read.csv (file="/home/kevin/autoglosser/profile/perprofile.csv",  na.strings = "NA", nrows = -1, skip = 0, check.names = TRUE, strip.white = FALSE, blank.lines.skip = TRUE) 
barplot(t(perprofile), col=c("mediumvioletred", "khaki1", "lightskyblue1"), space=0)
title(main="Conversation profile for sastre1")
dev.off()






Conversion to new CHAT default with precodes (from @0, @1, @2, @3 format)
=================================================
Run php utils/generate_lgprofile.php filename to find all utterances that need a precode.  Output: filename_lgprofile db table.
Run php utils/convert_to_precode.php  filename to inject the precode and switch the tags.  Output: filename_b.cha text file in outputs/filename dir.
Add header and footer.
Copy filename_b.cha into clan/chats (CLAN doesn't seem to be able to find any file if it's not in its dir).
Run php utils/run_mor_post.php filename to use CLAN's MOR and POST apps to generate a %mor tier.  Output: filename_b.cha in inputs dir
Run php import_only.php inputs/filename_b.cha to import the MOR-glossed text.
Remove --trace from apply_cg.php (or you get SUBSTITUTE, SELECT etc in some entries.
Run php autogloss_only.php sastre1_b to autogloss the MOR-glossed text.


select * from sastre1_b_cgwords where auto~'\\[or\\]' order by surface

select utterance_id, lgprofile from sastre1_lgprofile where lgprofile !~'3' and lgprofile != ''

total tokens: select count(*) from tablename
L2: select count(*) from tablename where mor ~ 'L2'
(L2): select count(surface) from tablename where mor ~ 'L2' and surface !~ '[A-Z]' and langid !='spa&eng' and surface !='.'
total types: select surface, langid, count(surface) from tablename where surface !~ '[A-Z]' and langid !='spa&eng' and langid !='eng&spa' and surface !='.' group by langid, surface order by langid, surface
(L2) types: select surface, langid, count(surface) from tablename where  mor ~ 'L2' and surface !~ '[A-Z]' and langid !='spa&eng' and langid !='eng&spa' and surface !='.' group by langid, surface order by langid, surface

                        sastre1                    sastre8                        sastre08_fixed
total tokens     6958                        6627                            6634
L2                    1205         18%         890          13%             170              3%
(L2)                  873           13%	        743          11%             165              3%
total types       1290                        1189                            1210
(L2) types         333          26%         252          21%             109              9%
TTR                   19%                         18%                             18%
English                                            3201                            3405 
Spanish                                           2080                            2148
Indeterminate                                  270                              0

The imported files should not contain a dash (hyphen) in their name - this is not a legal character in PostgreSQL table names.

When importing files which do not use the precode format, comment out the $mflg and $lflg lines in fns.php.



To wrangle tiers in a file
===============
grep '\*[A-Z]' sastre1.cha > sastre1_surface.cha - moves the surface tiers into a new file
grep -v '%mor' sastre1.cha > sastre1_aut.cha - moves all tiers except %mor into a new file (ie removes %mor tiers)


Diffing files
=======
diff sastre1orig.cha /home/kevin/data/miami/sastre1.cha > sastre1.diff


Getting the total number of different regions in two files
=================================
The lines that specify the changed regions start with numbers, so to get the total number of changed regions, do:
diff filea fileb | grep '^[1-9]' | wc -l
To get the total number of changed lines (even if they are contiguous within a region), do:
diff filea fileb | grep '^<' | wc -l


Final tweaking of files
=============

If manual changes need to be made to files, adjust the _cgwords entries directly.  Then run write_cgautogloss.php again to generate the new cha file.  Run generate_expex.php to create the new pdf, and process it via Kile in the normal way.


Notes for papers
===========

"prefer" option would be nice, ie prefer indicative to subjunctive or imperative

move from more specific to less specific:
select ([cy] "o" prep :of:) if (-1 ([cy] n)) (2 (n) or (pron));  # llun o nhw - needs to go before the next one
select ([cy] pron m 3s) if (-1 ([cy] n));  # enw fo

patagonia2:
------------------
mewn home 
yn nawdeg tri (in '93), but yn nawdeg tri [oed] (93 years old)
roedd y te fantástico
oedd hynna o. Mold - can we jump across sentence boundaries?
roedd hi yn sister

patagonia6:
------------------
dim_ond ~ mond ~ dim ond
yn perthyn i'r capel - i uses: select ([cy] pron) if (-1 inflected); because an inflected reading is in the cohort for perthyn; how to run the rule based on the selected reading (infin in this case)?  select ([cy] pron) if (-1 inflected)(not -1 (v infin)); works
yn ffrind i'r daith - yn is ambiguous
"<gallu>"
	"gallu" {496,5} [cy] v infin :be_able: [205233] SELECT:352
;	"gallu" {496,5} [cy] n m sg :capacity: [203242] REMOVE:104
;	"gallu" {496,5} [cy] n m sg :capability: [203243] SELECT:352
Removing an excess noun allows the yn rule to bite.
Combine in post-processor?
"<llosgi>"
	"llosgi" {587,1} [cy] v 2s pres :burn: [32810]
	"llosgi" {587,1} [cy] v infin :burn: [202266]
or:
"<yn>"
	"yn" {636,8} [cy] stat :stative: [200654]
	"yn" {636,8} [cy] prep :in: [204430]
;	"gan" {636,8} [cy] prep :with: [196964] + sm REMOVE:122
"<bachgen>"
	"bachgen" {636,9} [cy] n m sg :boy: [200585]
Setting up rule for notas allows det.def rule to bite
"<bod>"
	"bod" {633,23} [cy] v infin :be: [205519]
"<y>"
	"y" {633,24} [cy] det.def :the: [204199] SELECT:157
;	"y" {633,24} [cy] pron.rel :that: [200652] SELECT:157
"<notas>"
	"nota" {633,25} [es] n f pl :note: [80772] SELECT:719
;	"notar" {633,25} [es] v 2s pres :sense: [80773] SELECT:719
mynd i'r comisaría i gael dirección rhei yna


Watch out for typos in the grammar file - if you end up with absolutely no autoglosses, it is probably due to this.  If you are using the umbrella scripts, the warning will flash past to quick to see.  Compare the _cg and _cg_applied files - if the former is full and the latter is empty, there is a typo.  Running php apply_cg.php filename will confirm this.







Timings
=====

Siarad
----------
507m (8h27m) for 456k words, 40 hours of conversation
	1m to do 5m of conversation
	900 words a minute
Note that this includes importing and segmenting the gls tier - a bare file with speech tier would be faster

Patagonia
---------------
175m (2h55m) for 161k words, 20 hours of conversation
	1m to do 7m of conversation
	920 words a minute

Sampling
--------------
Sampling clauses for 68 files took 14m.


Features to add
==========

1 - Change name of final file from filename_autoglossed.txt to filename.cha.

2 - For indeterminate words, try lookup first in the English dictionary (or whatever) (using the demut functions), and then in inlist.
freak_o, whine_io: split at _[i]o and look up the first bit


Clause sampling
==========

The following 4 speakers are omitted from the sample (why is not entirely clear)
CLR		robert7
HUW		robert7
LNW		fusser12
TWM		robert7

select count(*) from (select speaker, count(clause) from all_siarad_clean group by speaker order by count) as foo where count>19 and count<30


Produce 6 slides to a page
=================

pdfnup --no-landscape --nup "2x3" --offset ".25cm .25cm" --delta ".25cm .5cm" --frame true --scale 0.9 isb8pres.pdf
There is better info in the Notebook.


Tagless files
========

If it is required to clean existing %eng tags out of the file: sed -i '/%eng/ d' inputs/miami/beta/sastre9.cha 
Run utils/sh_add_eng to add %eng tiers containing vvv.

Run php cgimport.php file to import the new tiers into the file_cgutterances table.
Run php write_tagless.php file to write out the utterances table with the %eng tiers replaced by the number of the utterance they should attach to.
Run utils/sh_remove_tags to remove tags.
Run unoconv -f doc miamitrans/file_tagless.txt to convert the txt file to a doc file.
Open the doc file and change the font to Times New Roman 12 pt. 

Setting up and applying a template with this font doesn't seem to work.  unoconv itself (http://dag.wieers.com/home-made/unoconv) seems a bit flaky too: 
unoconv -f rtf outputs/herring13/herring13_tagless.txt 
unoconv -f rtf outputs/herring13/herring13_tagless.odt
don't seem to work (seems to work before running sh_remove_tags, but not afterwards ...), and:
unoconv -f doc outputs/herring13/herring13_tagless.txt
sometimes doesn't work.  Actually, the error:
unoconv: UnoException during conversion: 
The provided document cannot be converted to the desired format.
seems to mean that a file of that name already exists, so unoconv is presumably refusing to overwrite it.  Delete the file, and the invocation works.
http://artofsolving.com/opensource/pyodconverter
http://www.oooninja.com/2008/02/batch-command-line-file-conversion-with.html
http://hardc0l2e.wordpress.com/2011/03/03/document-converter-for-multiple-files/

Edit utils/sh_add_eng for file and predominant language and then run it.
php cgimport.php inputs/miami/beta/sastre4.cha
php write_tagless.php sastre4
Edit utils/sh_remove_tags for file and then run it to remove language tags.
Run unoconv -f doc miamitrans/sastre9_tagless.txt to convert the txt file to a doc file.
Open the doc file and change the font to Times New Roman 12 pt. 
Post on website and let translators insert translations.

Edit translation.
Save as _tagless.txt
utils/sed_treat_doc
add header utterance_id (tab) eng

set up table _trans with above two fields
CREATE TABLE _trans (
    utterance_id integer,
    eng character varying(250)
);
import tabbed
update _trans set eng='vvv' where eng is null - to maintain record of utterances where no translation has been made

php cgimport.php inputs/miami/beta/sastre4.cha - to retain typo changes made
utils/sed_get_header inputs/miami/beta/sastre4.cha - to ensure we have the correct header for the write-out
update _cgutterances u set eng=(select eng from _trans t where t.utterance_id=u.utterance_id) - remember to change the filename in BOTH places; note that this will delete existing translations unless you add a where utterance_id > xxx at the end.
comment out section in write_cgautogloss.php and run it: php write_cgautogloss.php sastre4

check with Meld that the file is OK against the untranslated one
remove the Autoglosser line in the header
[cp outputs/sastre4/sastre4_autoglossed.txt inputs/miami/beta/sastre4.cha] - easier just to use Meld to copy to right


Silencing  the audio to ensure anonymity
==========================

create table patagonia36_pseud as select surface, count(surface) from patagonia36_cgwords where auto='name' group by surface
Edit this to remove placenames.
This step should be done automatically by referring to the pseudonym file.

sox input.wav output.wav pad duration@startpoint trim 0 endpoint duration splice startpoint endpoint  

sox silence_test.wav silence_kd.wav pad 2.601@113.878 trim 0 116.479 2.601 splice 113.878 116.479

sox input.wav output.wav pad duration@startpoint duration@startpoint ... trim 0 endpoint duration trim 0 endpoint duration ... splice startpoint endpoint startpoint endpoint ...

sox silence_test.wav silence_kd.wav pad 18.263@0.604 1.302@41.452 2.601@113.878 2.415@118.606 trim 0 18.867 18.263 trim 0 42.754 1.302 trim 0 116.479 2.601 trim 0 121.021 2.415 splice 0.604 18.867 41.452 42.754 113.878 116.479 118.606 121.021

At the end, you get warnings like:
WARN splice: Input audio too short; splices not made: 210
but the file seems to be silenced in the right places.

This is automated in anonymise_audio.php.


Running CHECK
==========

clan/unix/bin/check inputs/cypriot/beta/WS213275.cha 
Also need to run CHATTER


Abbreviations
=========

Select the POS section of the auto column.
Using substring:
select distinct substring(auto from '[A-Z].*') as auto_pos from combiwords_mi_uniq where auto !~'\[or\]' order by auto_pos limit 50

Using regexp_matches:
select distinct (regexp_matches(auto, E'[A-Z].*')) as auto_pos from combiwords_mi_uniq where auto !~'\[or\]' order by auto_pos limit 50
(Note that this gives the results in curly brackets, for some reason.)

Download the CSV file and remove the column header.

sed '/[A-Z][a-z].*/ d' < auto_pos_miami.txt | sed 's/"//g' | tr -sc '[A-Z][0-9]' '[\012*]' | sort | uniq | sed '/\]/ d' | sed '/\[/ d' > auto_pos_indiv_miami.txt
Add -c to uniq if you want counts.

Remove BE, GB, HAVE, ING, OK, PV, TO, TV, US

Remove ARCHAIC, AMER, ERR, LITERARY, NORTH, NSTAN, PAT, SHORT, VULG, SPOKEN
(Not needed for more recent glossings, where these are suppressed.)


Deleting full stop at the end of each %eng tier
=============================

sed -ri "/%eng/s/\.$//g" inputs/patagonia/beta/patagonia40.cha
And replace the stop where ... has become ..
sed -ri "/%eng/s/\.\.$/.../g" inputs/patagonia/beta/patagonia40.cha 

Delete the space at the end of the %aut tier:
sed -ri "/%aut/s/ $//g" inputs/patagonia/autoglossed/patagonia3.cha

Delete empty %aut tiers:
sed -ri "/%aut:\t$/ d" inputs/patagonia/autoglossed/patagonia3.cha



Get list of pseudonyms in each file_cgutterances
===============================

ack-grep '^\*[A-Z]{3}' inputs/patagonia/beta/patagonia40.cha | ack-grep -o '[A-Z][a-z].[^@ ]*' | sort | uniq > current40
Select the speaker tier in each file, select the name in each tier (note the use of -o to capture the match only, not the whole line), sort these, keep only a unique instance of each name, and save them to the file current40.


Errors
====

during write_cohorts.php:
Fatal error: Cannot redeclare query() (previously declared in /home/kevin/autoglosser/includes/fns.php:42) in /home/kevin/autoglosser/includes/fns.php on line 48
Testing code
//include("includes/fns.php");
//include("/opt/autoglosser/config.php");
//$surface="mirarélas";
had been left uncommented in lookups/es_lookup.php


Chatter
=====
This will not pass things that CHECK does pass:
- anything related to the word ( [?], [//], etc ) cannot have (.) etc between it and the word
- a ? in the @ID line is not allowed
- everyone in the @Participants line has to have a corresponding @ID entry
There sometimes seems to be a problem with the @End marker, which can be fixed simply by Backdelete and then Return.

User tiers (eg %aut) will have every character flagged as non-viable.  To avoid this, you need to recode %aut as %xaut, run Chatter, and then change %xaut back to %aut.


Autogloss correction
=============

If you adjust the dictionary or the cgwords table, you have to run write_cohorts.php again.
If you adjust the rules, you have to run apply_cg.php or apply_traced_cg.php again.

Get the existing [or]s and add them to the tidy_auto table, using the first segment of the [or] as the substitute (sub) for the [or] autogloss entry, and removing the trailing dot:
append_or.php

Edit the sub field as necessary, and remove "new" from the newentry field/

Read each [or], and if it is in the or-file, replace the auto in the file (with an asterisk appended to show it is the result of tidying):
tidy_or.php

Run append_or.php again as necessary, editing the sub field.

Use Meld to view changes throughout the whole file dependent on changes in the grammar.

Note that the tidy_or table will only work if the "value-based" tags remain omitted.  If they are added, the auto string will be different, and therefore the substitution will not work.


Second edition of Siarad
================

For the files in inputs/siarad we need to:
- remove the filename from the sound bullets
- fix the mistaken punctuation (revert_punctuation() in fns.php, originally used in utils/make_global_changes.php)
- change the @Languages line to give 3-letter abbreviations (and amend other parts of the header as well) - this can perhaps be done via utils/make_global_changes.php - IMPORTANT!!! once the beta files are fixed, we need to run utils/sh_run_prepare with the utils/sed_get_header $f line live, to collect the fixed headers - this is because the langid tag selection and the writeout uses the stored header - FIXME? perhaps it should just run anew each time from the beta file























