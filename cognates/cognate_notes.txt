Cognate work
=========

Get utterances containing more than 3 cognates, for testing purposes
---------------------------------------------------------------------------------------------------------------
create table cogloctest as select utterance_id from (select utterance_id, count(utterance_id) from stammers4_cogloc group by utterance_id order by utterance_id) as mine where count>3 order by count

create table stammers4_cogloctest as select * from stammers4_cogloc where utterance_id in (select utterance_id from cogloctest) order by utterance_id, location


Run gather_cognate_locations to pick out all the cognates in a file and put them in a cogloc table.
Run segment_at_cognate to split the utterances into segments at each cognate, based on the entries in the cogloc table.


November 2011
==========

extend_cgwords.php - adds to the table 4 new fields which are required for the analysis.

The first stage is to split the clauses - make_clauses.php runs these first 4 scripts:
(1) mark.php - puts a marker at the clause division - select the relevant line to choose either finite verbs only (for MLF analysis) or all verbs including infinitives (for cognate analysis)
(2) adjust_deletes.php - deletes the clause marker where necessary - uncomment the INFIN section for MLF analysis, where only finite verbs are required
(3) adjust_moves.php - moves the clause marker where required
(4) segment.php - segment the utterances into clauses - select the relevant line if you want to choose finite verbs only (for MLF analysis)

Then bracket the reinforcers, so that they are not counted when we count speech-turns.
reinforcers.php - marks the clauses to be ignored

Inject the cognates:
insert_triggers.php - mark the words in the file as cognates (triggers), based on the various cognate lists

write_rei.php - writes out the file leaving out clauses that consist only of reinforcers
Note that this script adds data to the table to indicate speech-turns and clauses in each speech turn, and the writing out is commented out, so that the name is actually a misnomer - it is actually a speaker turn counter.

(deprecated) adjust_t.php - previously used (based on adjust_deletes.php) to deal with things like fan and pan, which were being marked as derived from English even though they are Welsh; now handled by additional clauses in the SQL queries in insert_cognates.php

(deprecated) Apply the clause algorithm to count the switches for each trigger:
(deprecated) generate_clause_data.php - produces a TeX file with a running total

(now called from write_cognates.php) create_cognates.php - create a new table, filename_cognates, to hold data about each clause

write_cognates.php - get the relevant data from the filename_cgwords table and write it to the filename_cognates table

analyse_cognates.php - analyse the filename_cognates table and write the external and internal codeswitch data to that table using the following algorithms:
	external (interclausal):
		// get the language of the last non-T in this clause (call it p_lg)
		// get the language of the first non-T in the next clause (call it f_lg)
			// if f_lg != p_lg, then S, else NS
			// if this clause contains T, then T, else NT
	internal (intraclausal):
		// if there is more than one language in this clause, then S, else NS
			// if this clause contains T, then T, else NT

select (maxloc-minloc)+1 as clause_length, count((maxloc-minloc)+1) as frequency from stammers4_cognates where speaker='ELE' group by clause_length  order by clause_length 

results_table.php - read the trigger/codeswitch data from each table and add them to a cognate_results table.  This can then be fed into R.


Linear model (regression)
================

Extract clauses containing only triggers.

Classify the triggers in roberts1_regression.

Combine this with the codeswitch data:
create table roberts1_lm as select clause_id, spkturn, clspk, newturn, utterance_id, minloc, maxloc, t_ser, f_lg, p_lg, nt_lg_ser, external, externalb, internal, speaker, c.surface, auto, noun, "-io_verb", _io_verb, adjective, adverb, "IM", name from roberts1_cognates c, roberts1_regression r where c.surface=r.surface

Get the data for external CS:
select * from roberts1_lm where external='ST'
select * from roberts1_lm where external='NST'
Note that clauses where no codeswitch can be counted (---) are omitted.


May 2012: Clause analysis
=================

WARNING!!! The above will overwrite the _cognates tables, and therefore delete any info about codeswitches and triggers.  Codeswitch information with no reference to triggers (ie SNT and NSNT instead of SNT, ST, NSNT, NST) can be restored by running cognates/analyse_cognates.php, but the best long-term solution is probably to set up tables with different names eg (_clauses for this type of analysis).

Regloss.  Problem with @Languages in the header being "cy, en" instead of "cym, eng".  Adjusting the $mflg routine to allow "cy" and "en" imported the langid for Welsh as "cy" instead of "cym" (but "eng" was OK) - use utils/sh_pg_stuff to adjust the tables.  The Siarad headers probably need to be changed to reflect the fact that the new precode format is used.

php cognates/extend_cgwords.php davies9
php cognates/make_clauses.php davies9
php cognates/reinforcers.php davies9
php cognates/write_rei.php davies9 - this gives the speaker turns, and also a printout that we don't use here
php cognates/analyse_clauses.php davies9

The above are now all included in cognates/prep_rbrul.php, so the process can be launched over the corpus by editing and running cognates/sh_run_prep_rbrul.  Note that as well as outputting a file for each conversation, prep_rbrul.php appends each file to a "jumbo" file.  

You need to remove all instances of the headers:
sed -i '/^"speaker/ d' caroline/jumbo.csv 
and then put the first instance back in:
"speaker","utt_no","cl_in_utt","cl_start","cl_end","spkturn_no","cl_in_spkturn","file","surface","autogloss","matrix_lg","linguality","dv","verb_morph","qlang","dob","gender","age","work","brought_up","main_area","education","welsh_since","english_since","welsh_ability","english_ability","mother_spoke","father_spoke","guardian_spoke","primary_lg","secondary_lg","welsh_modern","welsh_useful","welsh_friendly","welsh_inspiring","welsh_beautiful","welsh_influential","english_modern","english_useful","english_friendly","english_inspiring","english_beautiful","english_influential","contact1","contact2","contact3","contact4","contact5","nat_id","i_separate","shdbe_separate"

Also remove and clauses assigned to the researcher:
sed -i '/^"RES/ d' caroline/jumbo.csv 



Getting codeswitch numbers from the cognate_results table
======================================

select filename, ext_na, ext_st+ext_snt as ext_switch, ext_nst+ext_nsnt as ext_noswitch, int_na, int_st+int_snt as int_switch, int_nst+int_nsnt as int_noswitch from cognate_results


Gathering -io verbs
============

create table di_ioverbs as select * from combiwords_siar_uniq where surface~'i?o$' and auto~'\.V\.' and langid~'(cym|cym&eng|cym&spa)$' order by surface

insert into di_ioverbs select * from combiwords_pat_uniq where surface~'i?o$' and auto~'\.V\.' and langid~'(cym|cym&eng|cym&spa)$' order by surface

select surface, auto from di_ioverbs group by surface, auto order by surface, auto

create table di_ioverbs_uniq as select surface, auto from di_ioverbs group by surface, auto order by surface, auto

insert into di_ioverbs_uniq (surface, auto) select surface, auto from combiwords_siar_uniq where surface~'i?an$' and auto~'\.V\.INFIN' and langid~'(cym|cym&eng|cym&spa)$' order by surface


Things to look at
===========

Does it make sense not to count code-switches between speaker-turns?

Reinsert reinforcers.

Split at infinitives too.




Trigger determination
==============

Collect all indeterminates from a file:
create table stammers4_triggers as select distinct surface from stammers4_cgwords where langid!='cym' and langid!='eng' and langid!='999' order by surface


Notes for technical description
====================

Import and gloss the files using the autoglosser.

Set up lists of words for the onion-rings.  Diana: discuss what was done here.

Split the clauses.  Used a split on finite verbs.  Will examine split on non-finite verbs as well in a second stage.  Discuss pros and cons.  How this was done.  Add a clause-marker to the word table.  Delete inappropriate clause-markers, and move ones that are incorrectly placed.  Rather a blunt knife.

Mark reinforcers, so that they are not counted when we count speech-turns.  Will also count them in a second stage.

Inject the cognates.  Multiple groups can be injected, based on the onion-ring lists.

Count the clause-switches.



Cypriot
====

Mark (on a preliminary basis) and segment the file :
cyp_mark.php - puts a marker at the clause division
cyp_deletes.php - deletes the clause marker where necessary
cyp_moves.php - moves the clause marker where required
cyp_segment.php - segment the utterances into clauses - this gives a _split file
cyp_clauses.php will run all four of these in succession.

Get someone to edit the for incorrect clause-splitting, using // for a split, and ++ for a join.

add_wordno.php to add a column to the $words table to number all the words (ie not counting the punctuation), and then fill the wordno column with serial numbers.

Correct the edited file manually to give an _import file, and then read it in with import_clauses.php.

Run cyp_segment.php again, and check the output (_split) against the edited (_import) files using Meld.  Note that this process is very dependent on there being no typos in the original chat file.  If there are (eg at @1, with a space between the word and the language tag), they will knock off the marking of the words.  You may therefore have to correct the file, and reimport it (several times if it is untidy!).  It would perhaps be worth running CHECK before import.  Another common sticking-point is omission of a punctuation mark at the end of the utterance, or +.. instead of +... (three dots).

cyp_write.php - writes out the imported file into a new _csdata table.
cyp_analyse.php - analyses the _csdata table, adds codeswitch instances to it, and writes out a text file with the codeswitch data


select speaker, count(external) from ws210021_csdata where external='ext' group by speaker
select speaker, count(internal) from ws210021_csdata where internal='int' group by speaker

select speaker, sum(external) as external, sum(internal) as internal from cyp_count group by speaker



Bugs
===

External forward shows ST or NST in the first line of some files.  This is because the first clause is not being marked "new" in those files.  Why?

A single (trigger?) word at the head of a speech-turn will be marked as external ST.






create table fusser28_cgwords_nuked as select * from fusser28_cgwords







































