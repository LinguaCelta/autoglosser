#!/bin/sh

#/* 
#*********************************************************************
#Copyright Kevin Donnelly 2010, 2011.
#kevindonnelly.org.uk
#This file is part of the Bangor Autoglosser.
#
#This program is free software: you can redistribute it and/or modify
#it under the terms of the GNU General Public License and the GNU
#Affero General Public License as published by the Free Software
#Foundation, either version 3 of the License, or (at your option)
#any later version.
#
#This program is distributed in the hope that it will be useful,
#but WITHOUT ANY WARRANTY; without even the implied warranty of
#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#GNU General Public License for more details.
#
#You should have received a copy of the GNU General Public License
#and the GNU Affero General Public License along with this program.
#If not, see <http://www.gnu.org/licenses/>.
#*********************************************************************
#*/ 

# This script does a compressed dump of the _cgutterances and _cgwords table for each file in the corpus into a dump dir.
# The --clean option means that the file will include SQL commands to drop the old table.
# Use sh_global_update_corpus to import these compressed dumps back into a database.

corpus="siarad"

#FILES="herring1 herring10 herring7 herring9 sastre1 zeledon1"  # Individual table names.
FILES=inputs/$corpus/beta/*  # Corpus tables, based on the chat files in the corpus directory.
GZDIR=dbdevel/${corpus}_dump  # Specify the dump dir.

mkdir -p $GZDIR

for f in $FILES
do
	filename=$(basename $f)  # the filename from the path - file.cha
	chat=${filename%.cha}  # drop the filename's extension
	
	echo "Processing $chat ..."
	pg_dump -t ${chat}_cgutterances --clean autoglosser | gzip > $GZDIR/${chat}_cgutterances.sql.gz
	pg_dump -t ${chat}_cgwords --clean autoglosser | gzip > $GZDIR/${chat}_cgwords.sql.gz
done
